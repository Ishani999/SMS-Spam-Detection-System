# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUhFgPwwg1pRAxyinkDZ49jJv4vZ_baF
"""
import pandas as pd
import numpy as np
import os  # Import the os module for file path operations

import nltk
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import pickle

# Load the dataset (assuming the file is named 'spam.csv')
try:
  df = pd.read_csv('spam.csv', encoding='latin-1')  # Try latin-1 encoding for Kaggle dataset
except FileNotFoundError:
  print("Error: 'spam.csv' file not found. Please download the dataset from Kaggle and place it in the same directory as your script.")
  exit()

# Assuming the first two columns (v1 and v2) contain the data you need
selected_df = df[['v1', 'v2']]  # Select only v1 and v2 columns

# Rename the columns if necessary (optional)
if 'label' not in selected_df.columns or 'message' not in selected_df.columns:
  selected_df.columns = ['label', 'message']  # Rename only if original names differ



selected_df.head()


nltk.download('stopwords')

# Initialize the Porter stemmer
ps = PorterStemmer()

# Function to clean the text
def preprocess_text(text):
  # Remove non-alphabet characters
  text = re.sub('[^a-zA-Z]', ' ', text)
  # Convert to lowercase
  text = text.lower()
  # Tokenize the text
  text = text.split()
  # Remove stopwords and stem the words
  text = [ps.stem(word) for word in text if word not in set(stopwords.words('english'))]
  # Join the words back into a single string
  text = ' '.join(text)
  return text
# Preprocessing with assignment
selected_df['message'] = selected_df['message'].apply(preprocess_text)

# Label encoding with assignment
selected_df['label'] = selected_df['label'].map({'ham': 0, 'spam': 1})

selected_df.head()

from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(selected_df['message'], selected_df['label'], test_size=0.2, random_state=42)


from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the training data
X_train_vec = vectorizer.fit_transform(X_train)

# Transform the test data
X_test_vec = vectorizer.transform(X_test)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train_vec, y_train)

from sklearn.metrics import classification_report, accuracy_score, make_scorer, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, cross_val_predict

# Perform 10-fold cross-validation
y_train_pred = cross_val_predict(rf_model, X_train_vec, y_train, cv=10)

# Print the classification report
print("10-Fold Cross-Validation Classification Report")
print(classification_report(y_train, y_train_pred))

# Print the accuracy score
print(f'10-Fold Cross-Validation Accuracy: {accuracy_score(y_train, y_train_pred)}')

# Train the model on the full training set
rf_model.fit(X_train_vec, y_train)

# Make predictions on the test data
y_test_pred = rf_model.predict(X_test_vec)

# Print the classification report for the test set
print("Test Set Classification Report")
print(classification_report(y_test, y_test_pred))

# Print the accuracy score for the test set
print(f'Test Set Accuracy: {accuracy_score(y_train, y_train_pred)}')

model_file_path = r'C:\Users\ishani anushka\Desktop\sms-spam-detection/rf_spam_classifier_model.pkl'
vectorizer_file_path = r'C:\Users\ishani anushka\Desktop\sms-spam-detection/tfidf_vectorizer.pkl'


# Save the model to a pickle file
with open(model_file_path, 'wb') as model_file:
  pickle.dump(rf_model, model_file)

with open(vectorizer_file_path, 'wb') as model_file:
  pickle.dump(vectorizer, model_file)

print(f"Model saved to: {model_file_path}")
